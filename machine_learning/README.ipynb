{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein DB - machine learning\n",
    "### Ghanyy, 106080\n",
    "#### 24 stycznia 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Purpose of this project is to create simple classification based on [Protein Data Bank](http://www.rcsb.org/) (PDB) data analysed previously using R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd # reading raw data, preprocessing\n",
    "import operator # used only for intelligent filtering\n",
    "import pickle # saving python array to file\n",
    "from sklearn.grid_search import RandomizedSearchCV # parameters parametrisation using random search\n",
    "from sklearn.ensemble import RandomForestClassifier # random forest classifier\n",
    "from sklearn.preprocessing import Imputer # clearing NaNs from data frame data\n",
    "from scipy.stats import randint as sp_randint # random numbers generation\n",
    "from sklearn.externals import joblib # saving best classifier to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading raw csv data\n",
    "### Following utility functions where implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Opening targeted file.\n",
    "def load_data_frame(file_path, column_separator, nan_text):\n",
    "    if file_path != '' and column_separator != '':\n",
    "        if nan_text != '':\n",
    "            df = pd.read_csv(file_path, sep=column_separator, na_values=nan_text, keep_default_na=False, dtype='unicode')\n",
    "        else:\n",
    "            df = pd.read_csv(file_path, sep=column_separator, dtype='unicode')\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        raise Exception('Mandatory parameters missing')\n",
    "\n",
    "\n",
    "# Filtering unwanted rows in selected data set column.\n",
    "def filter_unwated_rows(df, columns, values):\n",
    "    if df is not None and len(columns) > 0 and len(values) > 0:\n",
    "        if len(columns) == len(values):\n",
    "            filtered_df = df\n",
    "            filter_pairs = zip(columns, values)\n",
    "            for filter_pair in filter_pairs:\n",
    "                filtered_df = filtered_df[~df[filter_pair[0]].isin(filter_pair[1])]\n",
    "\n",
    "            return filtered_df\n",
    "        else:\n",
    "            raise Exception('Parameters mismatch')\n",
    "    else:\n",
    "        raise Exception('Mandatory parameters missing')\n",
    "\n",
    "        \n",
    "# Filtering only unique values of group columns.\n",
    "def filter_unique_groups(df, groups):\n",
    "    if df is not None and len(groups) > 0:\n",
    "        filtered_df = df\n",
    "        for group in groups:\n",
    "            filtered_df = filtered_df.drop_duplicates(subset=group)\n",
    "\n",
    "        return filtered_df\n",
    "    else:\n",
    "        raise Exception('Mandatory parameters missing')\n",
    "\n",
    "\n",
    "# Filtering rows with count threshold.\n",
    "# note: columns is string[], operators is operator[], tresholds is int[]\n",
    "def filter_count_treshold(df, columns, operators, tresholds):\n",
    "    if df is not None and len(columns) > 0 and len(tresholds) > 0 and len(operators) > 0:\n",
    "        if len(columns) == len(tresholds) == len(operators):\n",
    "            filtered_df = df\n",
    "            filter_pairs = zip(columns, operators, tresholds)\n",
    "            for filter_pair in filter_pairs:\n",
    "                filtered_df = filtered_df[filter_pair[1](df[filter_pair[0]].map(df[[filter_pair[0]]].stack().value_counts()), filter_pair[2])]\n",
    "\n",
    "            return filtered_df\n",
    "        else:\n",
    "            raise Exception('Parameters mismatch')\n",
    "    else:\n",
    "        raise Exception('Mandatory parameters missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loading PDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "# Load all_summary.txt from http://www.cs.put.poznan.pl/dbrzezinski/teaching/zed/zed_projekt_2015-2016_dane.7z\n",
    "data_frame = prep.load_data_frame(\"data/all_summary.txt\", \";\", \"nan\")\n",
    "\n",
    "# Delete rows with res_name equal to: “DA”,“DC”,“DT”, “DU”, “DG”, “DI”,“UNK”, “UNX”, “UNL”, “PR”, “PD”, “Y1”, “EU”, “N”, “15P”, “UQ”, “PX4” or “NAN”.\n",
    "data_frame = prep.filter_unwated_rows(data_frame, [\"res_name\"], [[\"DA\", \"DC\", \"DT\", \"DU\", \"DG\", \"DI\", \"UNK\", \"UNX\",\n",
    "                                                                      \"UNL\", \"PR\", \"PD\", \"Y1\", \"EU\", \"N\", \"15P\", \"UQ\",\n",
    "                                                                      \"PX4\", \"NAN\"]])\n",
    "\n",
    "# Filter only unique pairs of (pdb_code, res_name).\n",
    "data_frame = prep.filter_unique_groups(data_frame, [[\"pdb_code\", \"res_name\"]])\n",
    "\n",
    "# Discard res_name groups with count lesser than 5.\n",
    "data_frame = prep.filter_count_treshold(data_frame, [\"res_name\"], [operator.ge], [5])\n",
    "# len(data_frame) = 11005\n",
    "\n",
    "# Load grouped_res_name.txt which is two column file (11005 rows) containing (ln, grouped res_name's).\n",
    "data_frame_2 = prep.load_data_frame(\"data/grouped_res_name.txt\", \",\", \"\")\n",
    "\n",
    "# Load testing data frame and extract column names in order to extract data for learning process.\n",
    "test_df = prep.load_data_frame(\"data/test_data.txt\", \",\", \"nan\")\n",
    "# len(test_df) = 18917\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following utility functions were implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning values extracted directly from pandas data frame.\n",
    "def clean_data_values(data):\n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    imp.fit(data)\n",
    "    imp_data = imp.transform(data)\n",
    "\n",
    "    return imp_data\n",
    "\n",
    "\n",
    "# RandomForestClassifier creation using RandomizedSearchCV.\n",
    "def model_rfc(data, target, estimators_nbr, n_iter_search, joblib_dump_name):\n",
    "    # build a classifier\n",
    "    clf = RandomForestClassifier(n_estimators=estimators_nbr)\n",
    "\n",
    "    # specify parameters and distributions to sample from\n",
    "    param_dist = {\"max_depth\": [3, None],\n",
    "                  \"max_features\": sp_randint(1, 11),\n",
    "                  \"min_samples_split\": sp_randint(1, 11),\n",
    "                  \"min_samples_leaf\": sp_randint(1, 11),\n",
    "                  \"bootstrap\": [True, False],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "    # run randomized search\n",
    "    random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search,\n",
    "                                       scoring='recall_weighted')\n",
    "    random_search.fit(data, target)\n",
    "\n",
    "    # save best classifier to file\n",
    "    joblib.dump(clf, joblib_dump_name)\n",
    "    return random_search\n",
    "\n",
    "\n",
    "# Make extremely minimalistic prediction for given test set.\n",
    "def predict(model, test_data):\n",
    "    results = model.predict(test_data)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of RDB classifiers\n",
    "``` python\n",
    "# first parameter is empty (in fact it's an id), ignore it\n",
    "data_columns = list(test_df.columns.values)[1:]\n",
    "\n",
    "# extract only columns mentioned in test data set\n",
    "data_frame_training_data = data_frame.loc[:, data_columns].values\n",
    "\n",
    "# extract target data\n",
    "data_frame_target = data_frame.loc[:, \"res_name\"].tolist()\n",
    "data_frame_2_target = data_frame_2.loc[:, \"res_name_group\"].tolist()\n",
    "\n",
    "# clean data from NaNs\n",
    "data_frame_training_data = lrn.clean_data_values(data_frame_training_data)\n",
    "# verbose output: UserWarning: Deleting features without observed values:\n",
    "# [690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707\n",
    "#  708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725\n",
    "#  726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743\n",
    "#  744 745 746 747 748 749 750 751 752 753 754 755 756 757 758]\n",
    "\n",
    "# training and testing data sets must have matching columns, so it will be necessary to cut those off\n",
    "data_columns_exclude = [690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707,\n",
    "                                           708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725,\n",
    "                                           726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743,\n",
    "                                           744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 0]\n",
    "\n",
    "# Use RandomForestClassifier to teach simple res_name based classifier\n",
    "simple_model = lrn.model_rfc(data_frame_training_data, data_frame_target, 50, 50, \"data/tmp/simple_model.pkl\")\n",
    "\n",
    "# Use RandomForestClassifier to teach grouped res_name_group based classifier\n",
    "grouped_model = lrn.model_rfc(data_frame_training_data, data_frame_2_target, 50, 50, \"data/tmp/grouped_model.pkl\")\n",
    "\n",
    "# drop unnecessary columns and clean data\n",
    "test_df.drop(test_df.columns[data_columns_exclude], axis=1, inplace=True)\n",
    "test_data = test_df.values\n",
    "test_data = lrn.clean_data_values(test_data)\n",
    "\n",
    "# make predictions\n",
    "simple_prediction = lrn.predict(simple_model, test_data)\n",
    "grouped_prediction = lrn.predict(grouped_model, test_data)\n",
    "\n",
    "#save predictions to file\n",
    "pickle.dump(simple_prediction, open(\"data/tmp/simple_prediction.p\", \"wb\"))\n",
    "pickle.dump(grouped_prediction, open(\"data/tmp/grouped_prediction.p\", \"wb\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saved data\n",
    "In order to acuire saved data download .zip'ed catalog `data/tmp`.\n",
    "Predictions can be \"unpickled\" using `pickle` library.\n",
    "Classifiers can be loaded as `RandomForestClassifier` object with `sklearn.externals.joblib` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
